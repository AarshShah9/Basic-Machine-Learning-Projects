{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 4: Pipelines and Hyperparameter Tuning</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** = Aarsh Shah    \n",
        "* **UCID** = 30150079\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='Blue'>\n",
        "In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models, and evaluate the results. More details for each step can be found below. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T0uItvnoRoUB",
      "metadata": {
        "id": "T0uItvnoRoUB"
      },
      "source": [
        "<font color='Red'>\n",
        "For this assignment, in addition to your .ipynb file, please also attach a PDF file. To generate this PDF file, you can use the print function (located under the \"File\" within Jupyter Notebook). Name this file ENGG444_Assignment##__yourUCID.pdf (this name is similar to your main .ipynb file). We will evaluate your assignment based on the two files and you need to provide both.\n",
        "</font>\n",
        "\n",
        "\n",
        "|         **Question**         | **Point(s)** |\n",
        "|:----------------------------:|:------------:|\n",
        "|  **1. Preprocessing Tasks**  |              |\n",
        "|              1.1             |       2      |\n",
        "|              1.2             |       2      |\n",
        "|              1.3             |       4      |\n",
        "| **2. Pipeline and Modeling** |              |\n",
        "|              2.1             |       3      |\n",
        "|              2.2             |       6      |\n",
        "|              2.3             |       5      |\n",
        "|              2.4             |       3      |\n",
        "|     **3. Bonus Question**    |     **2**    |\n",
        "|           **Total**          |    **27**    |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OpeMjIV9VLgM",
      "metadata": {
        "id": "OpeMjIV9VLgM"
      },
      "source": [
        "## **0. Dataset**\n",
        "\n",
        "This data is a subset of the **Heart Disease Dataset**, which contains information about patients with possible coronary artery disease. The data has **14 attributes** and **294 instances**. The attributes include demographic, clinical, and laboratory features, such as age, sex, chest pain type, blood pressure, cholesterol, and electrocardiogram results. The last attribute is the **diagnosis of heart disease**, which is a categorical variable with values from 0 (no presence) to 4 (high presence). The data can be used for **classification** tasks, such as predicting the presence or absence of heart disease based on the other attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "YiaUdCQYVWj-",
      "metadata": {
        "id": "YiaUdCQYVWj-"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>120.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>160.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>130.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>155.0</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>180.0</td>\n",
              "      <td>393.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     28    1   2     130.0  132.0  0.0      2.0    185.0    0.0      0.0   \n",
              "1     29    1   2     120.0  243.0  0.0      0.0    160.0    0.0      0.0   \n",
              "2     29    1   2     140.0    NaN  0.0      0.0    170.0    0.0      0.0   \n",
              "3     30    0   1     170.0  237.0  0.0      1.0    170.0    0.0      0.0   \n",
              "4     31    0   2     100.0  219.0  0.0      1.0    150.0    0.0      0.0   \n",
              "..   ...  ...  ..       ...    ...  ...      ...      ...    ...      ...   \n",
              "289   52    1   4     160.0  331.0  0.0      0.0     94.0    1.0      2.5   \n",
              "290   54    0   3     130.0  294.0  0.0      1.0    100.0    1.0      0.0   \n",
              "291   56    1   4     155.0  342.0  1.0      0.0    150.0    1.0      3.0   \n",
              "292   58    0   2     180.0  393.0  0.0      0.0    110.0    1.0      1.0   \n",
              "293   65    1   4     130.0  275.0  0.0      1.0    115.0    1.0      1.0   \n",
              "\n",
              "     slope  ca  thal  num  \n",
              "0      NaN NaN   NaN    0  \n",
              "1      NaN NaN   NaN    0  \n",
              "2      NaN NaN   NaN    0  \n",
              "3      NaN NaN   6.0    0  \n",
              "4      NaN NaN   NaN    0  \n",
              "..     ...  ..   ...  ...  \n",
              "289    NaN NaN   NaN    1  \n",
              "290    2.0 NaN   NaN    1  \n",
              "291    2.0 NaN   NaN    1  \n",
              "292    2.0 NaN   7.0    1  \n",
              "293    2.0 NaN   NaN    1  \n",
              "\n",
              "[294 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data source link\n",
        "_link = 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data'\n",
        "\n",
        "# Read the CSV file into a Pandas DataFrame, considering '?' as missing values\n",
        "df = pd.read_csv(_link, na_values='?',\n",
        "                 names=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs',\n",
        "                        'restecg', 'thalach', 'exang', 'oldpeak', 'slope',\n",
        "                        'ca', 'thal', 'num'])\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mlcrJpGLWBOH",
      "metadata": {
        "id": "mlcrJpGLWBOH"
      },
      "source": [
        "# **1. Preprocessing Tasks**\n",
        "\n",
        "- **1.1** Find out which columns have more than 60% of their values missing and drop them from the data frame. Explain why this is a reasonable way to handle these columns. **(2 Points)**\n",
        "\n",
        "- **1.2** For the remaining columns that have some missing values, choose an appropriate imputation method to fill them in. You can use the `SimpleImputer` class from `sklearn.impute` or any other method you prefer. Explain why you chose this method and how it affects the data. **(2 Points)**\n",
        "\n",
        "- **1.3** Assign the `num` column to the variable `y` and the rest of the columns to the variable `X`. The `num` column indicates the presence or absence of heart disease based on the angiographic disease status of the patients. Create a `ColumnTransformer` object that applies different preprocessing steps to different subsets of features. Use `StandardScaler` for the numerical features, `OneHotEncoder` for the categorical features, and `passthrough` for the binary features. List the names of the features that belong to each group and explain why they need different transformations. You will use this `ColumnTransformer` in a pipeline in the next question. **(4 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yyRJQ25hXHNF",
      "metadata": {
        "id": "yyRJQ25hXHNF"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.1** This is a reasonable way to handle these columns because if a column has more than 60% of its values missing, it is not useful for analysis. Even if we fill in the missing values, the data will be biased and not accurate. Therefore, it is better to drop these columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "NzUkBHBfYBzF",
      "metadata": {
        "id": "NzUkBHBfYBzF"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>185.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>120.0</td>\n",
              "      <td>243.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>160.0</td>\n",
              "      <td>331.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>130.0</td>\n",
              "      <td>294.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>155.0</td>\n",
              "      <td>342.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>180.0</td>\n",
              "      <td>393.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>65</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>275.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     age  sex  cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  num\n",
              "0     28    1   2     130.0  132.0  0.0      2.0    185.0    0.0      0.0    0\n",
              "1     29    1   2     120.0  243.0  0.0      0.0    160.0    0.0      0.0    0\n",
              "2     29    1   2     140.0    NaN  0.0      0.0    170.0    0.0      0.0    0\n",
              "3     30    0   1     170.0  237.0  0.0      1.0    170.0    0.0      0.0    0\n",
              "4     31    0   2     100.0  219.0  0.0      1.0    150.0    0.0      0.0    0\n",
              "..   ...  ...  ..       ...    ...  ...      ...      ...    ...      ...  ...\n",
              "289   52    1   4     160.0  331.0  0.0      0.0     94.0    1.0      2.5    1\n",
              "290   54    0   3     130.0  294.0  0.0      1.0    100.0    1.0      0.0    1\n",
              "291   56    1   4     155.0  342.0  1.0      0.0    150.0    1.0      3.0    1\n",
              "292   58    0   2     180.0  393.0  0.0      0.0    110.0    1.0      1.0    1\n",
              "293   65    1   4     130.0  275.0  0.0      1.0    115.0    1.0      1.0    1\n",
              "\n",
              "[294 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 1.1\n",
        "# Add necessary code here.\n",
        "df.dropna(thresh=0.6*len(df), axis=1, inplace=True)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkk6IDQRXgJM",
      "metadata": {
        "id": "xkk6IDQRXgJM"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.2** \n",
        "\n",
        "- I chose to use the SimpleImputer and use 2 different methods for the different data types. \n",
        "    - For numerical columns which include columns like 'age', 'trestbps', 'oldpeak', etc I imputed the values using mean. I chose to do mean because it was simple and after visualizing the dataset it doesn't seem like there were that many outliers (vs median).\n",
        "    - For 'sex', 'cp', 'fbs', etc, which are represented as integers but are categorical or binary in nature, imputing with the most frequent value (mode) makes sense. This approach assumes the most common category is the most likely value when the actual value is missing.\n",
        "\n",
        "- This affects the data in two different ways:\n",
        "    - The numerical imputation preserves the overall distribution of the numerical data. In this case, these changes alter the variance and might possibly introduce bias if the data is not missing at random (MAR).\n",
        "    - For the categorical imputation, it doesn't introduce any new categories keeping things straightforward, but it may increase the prevalence of the most common category, potentially skewing the distribution, especially if the data is not MAR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "t7Hw48YkZcCb",
      "metadata": {
        "id": "t7Hw48YkZcCb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130.0</td>\n",
              "      <td>132.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>185.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>29.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>120.0</td>\n",
              "      <td>243.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>140.0</td>\n",
              "      <td>250.848708</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>237.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>170.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>100.0</td>\n",
              "      <td>219.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>150.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>52.0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>160.0</td>\n",
              "      <td>331.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>94.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>54.0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>130.0</td>\n",
              "      <td>294.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>100.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>56.0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>155.0</td>\n",
              "      <td>342.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>58.0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>180.0</td>\n",
              "      <td>393.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>110.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>65.0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>275.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>294 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      age  sex  cp  trestbps        chol  fbs  restecg  thalach  exang  \\\n",
              "0    28.0    1   2     130.0  132.000000    0        2    185.0      0   \n",
              "1    29.0    1   2     120.0  243.000000    0        0    160.0      0   \n",
              "2    29.0    1   2     140.0  250.848708    0        0    170.0      0   \n",
              "3    30.0    0   1     170.0  237.000000    0        1    170.0      0   \n",
              "4    31.0    0   2     100.0  219.000000    0        1    150.0      0   \n",
              "..    ...  ...  ..       ...         ...  ...      ...      ...    ...   \n",
              "289  52.0    1   4     160.0  331.000000    0        0     94.0      1   \n",
              "290  54.0    0   3     130.0  294.000000    0        1    100.0      1   \n",
              "291  56.0    1   4     155.0  342.000000    1        0    150.0      1   \n",
              "292  58.0    0   2     180.0  393.000000    0        0    110.0      1   \n",
              "293  65.0    1   4     130.0  275.000000    0        1    115.0      1   \n",
              "\n",
              "     oldpeak  num  \n",
              "0        0.0    0  \n",
              "1        0.0    0  \n",
              "2        0.0    0  \n",
              "3        0.0    0  \n",
              "4        0.0    0  \n",
              "..       ...  ...  \n",
              "289      2.5    1  \n",
              "290      0.0    1  \n",
              "291      3.0    1  \n",
              "292      1.0    1  \n",
              "293      1.0    1  \n",
              "\n",
              "[294 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 1.2\n",
        "# Add necessary code here.\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Assuming all columns except 'num' might have missing values\n",
        "features = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak']\n",
        "\n",
        "# For integer columns, impute with mean and round to the nearest integer\n",
        "float_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "int_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang']\n",
        "\n",
        "# Imputation\n",
        "imputer_float = SimpleImputer(strategy='mean')\n",
        "imputer_int = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "df[float_cols] = imputer_float.fit_transform(df[float_cols])\n",
        "df[int_cols] = imputer_int.fit_transform(df[int_cols])\n",
        "\n",
        "# Ensure integer columns are cast back to integers in case they were converted to float by imputation\n",
        "df[int_cols] = df[int_cols].astype(int)\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TS8GSVmmXoOg",
      "metadata": {
        "id": "TS8GSVmmXoOg"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **1.3**\n",
        "\n",
        "1. Numerical Features:\n",
        "    - Features: 'age', 'trestbps', 'chol', 'thalach', 'oldpeak'\n",
        "    - Transformation: 'StandardScaler'\n",
        "    - Since these are all continuous variables and can have varying ranges and distributions, Standard scaling (z-score normalization) is applied to ensure that these features have a mean of 0 and a standard deviation of 1. This helps prevent models that are sensitive to the scale of input features from misinterpreting the data.\n",
        "\n",
        "2. Categorical Features:\n",
        "    - Features: 'cp', 'restecg'\n",
        "    - Transformation: 'OneHotEncoder'\n",
        "    - Since these features represent categorical data that cannot be interpreted directly by machine learning models because they do not have an inherent numerical relationship. One-hot encoding transforms these categorical values into a binary matrix representation, ensuring that the model interprets these features correctly without assuming any ordinal relationship where it does not exist.\n",
        "\n",
        "3. Binary Features:\n",
        "    - Features: 'sex', 'fbs', 'exang'\n",
        "    - Transformation: None\n",
        "    - Since these features are already in a binary format (0 or 1), representing two categories in each variable and hence do not require normalization or one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "pCxLaTmQXYC7",
      "metadata": {
        "id": "pCxLaTmQXYC7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-2.54234669, -0.14707632, -1.83302736, ...,  1.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-2.41411716, -0.71634131, -0.1210522 , ...,  1.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [-2.41411716,  0.42218868,  0.        , ...,  1.        ,\n",
              "         0.        ,  0.        ],\n",
              "       ...,\n",
              "       [ 1.04808013,  1.27608618,  1.40584457, ...,  1.        ,\n",
              "         1.        ,  1.        ],\n",
              "       [ 1.30453919,  2.69924868,  2.19242775, ...,  0.        ,\n",
              "         0.        ,  1.        ],\n",
              "       [ 2.2021459 , -0.14707632,  0.37249019, ...,  1.        ,\n",
              "         0.        ,  1.        ]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1.3\n",
        "# Add necessary code here.\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "y = df['num']\n",
        "X = df.drop('num', axis=1)\n",
        "\n",
        "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
        "categorical_features = ['cp', 'restecg']\n",
        "binary_features = ['sex', 'fbs', 'exang']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
        "        ('pass', 'passthrough', binary_features),\n",
        "    ])\n",
        "\n",
        "X_transformed = preprocessor.fit_transform(X)\n",
        "X_transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a245d00",
      "metadata": {
        "id": "2a245d00"
      },
      "source": [
        "# **2. Pipeline and Modeling**\n",
        "\n",
        "- **2.1** Create **three** `Pipeline` objects that take the column transformer from the previous question as the first step and add one or more models as the subsequent steps. You can use any models from `sklearn` or other libraries that are suitable for binary classification. For each pipeline, explain **why** you selected the model(s) and what are their **strengths and weaknesses** for this data set. **(3 Points)**\n",
        "\n",
        "- **2.2** Use `GridSearchCV` to perform a grid search over the hyperparameters of each pipeline and find the best combination that maximizes the cross-validation score. Report the best parameters and the best score for each pipeline. Then, update the hyperparameters of each pipeline using the best parameters from the grid search. **(6 Points)**\n",
        "\n",
        "- **2.3** Form a stacking classifier that uses the three pipelines from the previous question as the base estimators and a meta-model as the `final_estimator`. You can choose any model for the meta-model that is suitable for binary classification. Explain **why** you chose the meta-model and how it combines the predictions of the base estimators. Then, use `StratifiedKFold` to perform a cross-validation on the stacking classifier and present the accuracy scores and F1 scores for each fold. Report the mean and the standard deviation of each score in the format of `mean ± std`. For example, `0.85 ± 0.05`. Interpret the results and compare them with the baseline scores from the previous assignment. **(5 Points)**\n",
        "\n",
        "- **2.4**: Interpret the final results of the stacking classifier and compare its performance with the individual models. Explain how stacking classifier has improved or deteriorated the prediction accuracy and F1 score, and what are the possible reasons for that. **(3 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GSpSIu-BY1Kn",
      "metadata": {
        "id": "GSpSIu-BY1Kn"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- **2.1** Create **three** `Pipeline` objects that take the column transformer from the previous question as the first step and add one or more models as the subsequent steps. You can use any models from `sklearn` or other libraries that are suitable for binary classification. For each pipeline, explain **why** you selected the model(s) and what are their **strengths and weaknesses** for this data set. **(3 Points)**\n",
        "\n",
        "\n",
        "Selected Models:\n",
        "\n",
        "- Logistic Regression: I chose this because in general this problem is inherently a binary classification problem and performs well when the data is linearly separable. It is able to predict well for simple, interpretable problems, which aligns quite well with this dataset.\n",
        "    - Strengths:\n",
        "        - It provides a probabilistic understanding of the predictions which is very useful for medical diagnoses \n",
        "        - It is efficient and offers quick training and prediction times, which is beneficial for a dataset with a moderate number of features and samples (which this dataset has)\n",
        "    - Weaknesses: In the case that the relationship here is much more complex, (which this dataset actually does have the potential to be -- see svc) than just linearly it can be slightly outperformed by something more complex. This may also mean that the model will be underfitting since a simpler model won't be able to reflect more complex relationships.\n",
        "\n",
        "- KNN (K Neighbour Classification): I chose KNN, specifically because it performed better than the Random Forest model (accuracy was 70%+ vs 80%+). In general, however, I chose it because it was a non-parametric method that was also simple and intuitive to understand. It took away most of the underlying assumptions about the distribution of the data.\n",
        "    - Strengths: No assumptions about the data distribution and easily adaptable to the disease dataset.\n",
        "    - Weaknesses: Here the weaknesses have the potential to be sensitive to scaling, and if that wasn't done properly in part 1, it could cause it to underperform.\n",
        "\n",
        "- SVC: I selected this because the heart disease dataset has a relatively high-dimensional space. And in this case, since the relationship is quite unknown it is able to perform well by performing linear and non-linear classification through the kernel trick.\n",
        "    - Strengths: As mentioned in this case it is effective and very capable of defining complex higher-dimensional boundaries for classification.\n",
        "    - Weaknesses: It requires careful tuning of the parameters like the choice of kernel and 'c' parameter. However, we are attempting to find the best values by using the pipeline so the main weakness here is computationally complexity to compute all the different values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "qYMtXgFtOBMT",
      "metadata": {
        "id": "qYMtXgFtOBMT"
      },
      "outputs": [],
      "source": [
        "# 2.1\n",
        "# Add necessary code here.\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define pipelines with different classifiers\n",
        "pipeline_lr = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "pipeline_knn = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "pipeline_svc = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC())\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NPSo4pBVe1GR",
      "metadata": {
        "id": "NPSo4pBVe1GR"
      },
      "source": [
        "- **2.2** Use `GridSearchCV` to perform a grid search over the hyperparameters of each pipeline and find the best combination that maximizes the cross-validation score. Report the best parameters and the best score for each pipeline. Then, update the hyperparameters of each pipeline using the best parameters from the grid search. **(6 Points)**\n",
        "\n",
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- Logistic Regression:\n",
        "    - Best Params:\n",
        "        - C: 0.1\n",
        "    - Score = 0.82\n",
        "    - F1 = 0.72\n",
        "\n",
        "- K-Nearest Neighbour:\n",
        "    - Best Params:\n",
        "        - N: 7\n",
        "        - P: 2\n",
        "        - Weights: Uniform\n",
        "    - Score = 0.82\n",
        "    - F1 = 0.74\n",
        "\n",
        "- Support Vector Machine:\n",
        "    - Best Params\n",
        "        - C: 0.1\n",
        "        - Gamma: 0.01\n",
        "        - Kernel: Linear\n",
        "    - Score = 0.82\n",
        "    - F1 = 0.73"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "sNXYl9WFe3vA",
      "metadata": {
        "id": "sNXYl9WFe3vA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters for LR: {'classifier__C': 0.1}\n",
            "Best accuracy for LR: 0.8196960841613091\n",
            "Best F1 score for LR: 0.7201635703043407\n",
            "Best parameters for KNN: {'classifier__n_neighbors': 7, 'classifier__p': 2, 'classifier__weights': 'uniform'}\n",
            "Best accuracy for KNN: 0.8230859146697839\n",
            "Best F1 score for KNN: 0.739776993969735\n",
            "Best parameters for SVC: {'classifier__C': 0.1, 'classifier__gamma': 0.01, 'classifier__kernel': 'linear'}\n",
            "Best accuracy for SVC: 0.8231443600233781\n",
            "Best F1 score for SVC: 0.7346270559734795\n"
          ]
        }
      ],
      "source": [
        "# 2.2\n",
        "# Add necessary code here.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# Define parameter grids for each classifier\n",
        "param_grid_lr = {\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "}\n",
        "\n",
        "param_grid_knn = {\n",
        "    'classifier__n_neighbors': [3, 5, 7, 9],\n",
        "    'classifier__weights': ['uniform', 'distance'],\n",
        "    'classifier__p': [1, 2],\n",
        "}\n",
        "\n",
        "param_grid_svc = {\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__kernel': ['linear', 'rbf'],\n",
        "    'classifier__gamma': [0.01, 0.1, 1, 10, 100],\n",
        "}\n",
        "\n",
        "scoring = {'accuracy': make_scorer(accuracy_score), 'f1_score': make_scorer(f1_score)}\n",
        "\n",
        "# Create GridSearchCV objects for each pipeline\n",
        "grid_search_lr = GridSearchCV(pipeline_lr, param_grid_lr, cv=5, scoring=scoring, refit='accuracy')\n",
        "grid_search_knn = GridSearchCV(pipeline_knn, param_grid_knn, cv=5, scoring=scoring, refit='accuracy')\n",
        "grid_search_svc = GridSearchCV(pipeline_svc, param_grid_svc, cv=5, scoring=scoring, refit='accuracy')\n",
        "\n",
        "\n",
        "# Fit each grid search to find the best model\n",
        "grid_search_lr.fit(X, y)\n",
        "grid_search_knn.fit(X, y)\n",
        "grid_search_svc.fit(X, y)\n",
        "\n",
        "# Print out the best parameters and scores for each model\n",
        "print(\"Best parameters for LR:\", grid_search_lr.best_params_)\n",
        "print(\"Best accuracy for LR:\", grid_search_lr.cv_results_['mean_test_accuracy'].max())\n",
        "print(\"Best F1 score for LR:\", grid_search_lr.cv_results_['mean_test_f1_score'][grid_search_lr.best_index_])\n",
        "\n",
        "print(\"Best parameters for KNN:\", grid_search_knn.best_params_)\n",
        "print(\"Best accuracy for KNN:\", grid_search_knn.cv_results_['mean_test_accuracy'].max())\n",
        "print(\"Best F1 score for KNN:\", grid_search_knn.cv_results_['mean_test_f1_score'][grid_search_knn.best_index_])\n",
        "\n",
        "print(\"Best parameters for SVC:\", grid_search_svc.best_params_)\n",
        "print(\"Best accuracy for SVC:\", grid_search_svc.cv_results_['mean_test_accuracy'].max())\n",
        "print(\"Best F1 score for SVC:\", grid_search_svc.cv_results_['mean_test_f1_score'][grid_search_svc.best_index_])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ygOeNB-PamnU",
      "metadata": {
        "id": "ygOeNB-PamnU"
      },
      "source": [
        "- **2.3** Form a stacking classifier that uses the three pipelines from the previous question as the base estimators and a meta-model as the `final_estimator`. You can choose any model for the meta-model that is suitable for binary classification. Explain **why** you chose the meta-model and how it combines the predictions of the base estimators. Then, use `StratifiedKFold` to perform a cross-validation on the stacking classifier and present the accuracy scores and F1 scores for each fold. Report the mean and the standard deviation of each score in the format of `mean ± std`. For example, `0.85 ± 0.05`. Interpret the results and compare them with the baseline scores from the previous assignment. **(5 Points)**\n",
        "\n",
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "Chosen meta-model: Given the problem being about medical diagnostic regarding heart disease it would make the most sense if the final estimator was one that could look at all the data and make a prediction based on probability. Thus I chose logistic regression because it not only allows us to estimate based on probability but can aggregate the predictions from the other models and capture all of the different underlying patterns weighing them accordingly. In lecture, we also saw that Logistic Regression was the most efficient and simple to understand so I went with it so I could iterate with a balance between performance and outcome.\n",
        "\n",
        "How it combines with the base estimators: The stacking classifier works by initially training the base estimators on the training data, then predicting each of them will provide its own prediction. These predictions are then used as input features for the meta-model. In the case of Logistic Regression as the meta-model, it treats these predictions as input variables and learns how to best combine them to make a final prediction. In essence, it's learning the optimal weighting of the base estimators' predictions based on their performance.\n",
        "\n",
        "\n",
        "Interpreting the results from the three different models, they all seem to have almost the exact same performance (deviating by max 1% on either accuracy or f1 score) thus they seem to perform generally well on the dataset thus there must be a relatively strong relationship between the features and the outcome and it must be simple enough that any of the models can easily pick up on those patterns. Making a comparison to the baseline score from the previous assignment we can delve deeper into how each model performed compared to the given data set. For example looking at the f1 scores of the wine dataset for 3 different models (SVC: 0.94, DT: 0.86, LSVC: 0.76) they all different by a substantial amount (up to 18% between SVC and LVC) which tells us that the wine dataset might have had some complicated relationships that SVC was able to model but the other models weren't able to. However, it is also important to note that the worst model in the wine dataset was better than all of the models for the heart disease dataset. This may mean that more feature engineering and data are required for this dataset + model combination to excel. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "UvhsbjmYP2G_",
      "metadata": {
        "id": "UvhsbjmYP2G_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.82 ± 0.06\n",
            "F1 Score: 0.74 ± 0.09\n"
          ]
        }
      ],
      "source": [
        "# 2.3\n",
        "# Add necessary code here.\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "# Define the stacking classifier\n",
        "estimators = [\n",
        "    ('lr', grid_search_lr.best_estimator_),\n",
        "    ('knn', grid_search_knn.best_estimator_),\n",
        "    ('svc', grid_search_svc.best_estimator_)\n",
        "]\n",
        "\n",
        "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
        "\n",
        "# Perform cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "acc_scores = cross_val_score(stacking_classifier, X, y, cv=cv, scoring='accuracy')\n",
        "f1_scores = cross_val_score(stacking_classifier, X, y, cv=cv, scoring='f1')\n",
        "\n",
        "# Report the scores\n",
        "print(f\"Accuracy: {acc_scores.mean():.2f} ± {acc_scores.std():.2f}\")\n",
        "print(f\"F1 Score: {f1_scores.mean():.2f} ± {f1_scores.std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A-TN9hr3b77-",
      "metadata": {
        "id": "A-TN9hr3b77-"
      },
      "source": [
        "- **2.4** Interpret the final results of the stacking classifier and compare its performance with the individual models. Explain how stacking classifier has improved or deteriorated the prediction accuracy and F1 score, and what are the possible reasons for that. **(3 Points)**\n",
        "\n",
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "The Mean and STD here is:\n",
        "Accuracy: 0.82 ± 0.06\n",
        "F1 Score: 0.74 ± 0.09\n",
        "\n",
        "- Looking at the results of the stacking classifier specifically the accuracy score it seems to perform quite well at 82%, however, the standard deviation is 6% which tells us that there is a decent amount of variability in the model performance across different subsets of data, thus the distribution of data is important as the predictive power may fluctuate based on that. In addition, the F1 score which is at 74% gives us an indication of the false negatives and false positives. In medical situations this is critical and 74% is generally quite bad since missing a diagnosis (false negative) could be the difference between life and death. A lower F1 score shows that the model cannot properly balance between recall and precision. \n",
        "\n",
        "- It seems as if the stacking classifier hasn't really improved nor degraded the results compared to all of the individual model results. Specifically, it could be because the base models make similar predictions (i.e., they are highly correlated), stacking may not add much value. The ensemble's strength comes from leveraging the diversity of predictions; if all models agree most of the time, the meta-learner has little room to improve. Another reason could be because the stacking classifier or any of the individual models could be overfitting or underfitting making it difficult for the stacking classifier to improve. Lastly, it could also be because all 3 of the individual models performed very similarly to each other thus there was no diversity to improve from."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RPa-v8Xxc7aU",
      "metadata": {
        "id": "RPa-v8Xxc7aU"
      },
      "source": [
        "**Bonus Question**: The stacking classifier has achieved a high accuracy and F1 score, but there may be still room for improvement. Suggest **two** possible ways to improve the modeling using the stacking classifier, and explain **how** and **why** they could improve the performance. **(2 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IrSooo0DfC-V",
      "metadata": {
        "id": "IrSooo0DfC-V"
      },
      "source": [
        "<font color='Green'><b>Answer:</b></font>\n",
        "\n",
        "- Optimize the Meta-Model: In this case, I chose to use LogisticRegression with no tuning as the meta-model since it was simple. However, if we wanted to improve the performance we could likely fine-tune it or choose a much more complicated model experimenting with models such as a gradient-boosted ensemble. This might allow us to leverage the strengths of multiple approaches instead of using LogisticRegression twice (once in the base estimators and again in the final). Specifically, this could have an impact because a more complex model might do a better job of capturing the intricate interactions between the base models, which is something the simple logistic regression could be missing.\n",
        "\n",
        "- Tuning the Stacking Classifier itself: Here I just used a very basic version of the stacking classifier by passing it to the base estimators and final estimators but its performance could be improved by experimenting with different configurations and changing how the base models are combined and by even adding more reducing the number of base models. For example, we could enable passthrough to allow the original features to be passed to the final estimator. Or change the value of 'cv' to change the number of folds for the cross-validation. In general, this could improve the performance because there could be a nuanced combination that works really well. Say one of the original features actually has a very strong relationship to the target variable, by enabling passthrough we allow it to have more feature importance thus improving the model.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
