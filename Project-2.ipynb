{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "<font size=\"+3\"><b>Assignment 2: Linear Models and Validation Metrics</b></font>\n",
        "\n",
        "***\n",
        "* **Full Name** = Aarsh Shah\n",
        "* **UCID** = 30150079\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "<font color='blue'>In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment.</font>\n",
        "\n",
        "You can use the Table of Content on the left side of this notebook to efficiently navigate within this documents.\n",
        "\n",
        "|                **Question**                | **Point** |\n",
        "|:------------------------------------------:|:---------:|\n",
        "|         **Part 1: Classification**         |           |\n",
        "|          Step 0: Import Libraries          |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    1.5    |\n",
        "| Step 3: Implement Machine Learning   Model |           |\n",
        "|           Step 4: Validate Model           |           |\n",
        "|          Step 5: Visualize Results         |     4     |\n",
        "|                  Questions                 |     4     |\n",
        "|             Process Description            |     4     |\n",
        "|           **Part 2: Regression**           |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    0.5    |\n",
        "| Step 3: Implement Machine Learning   Model |     1     |\n",
        "|            Step 4: Validate Mode           |     1     |\n",
        "|          Step 5: Visualize Results         |     1     |\n",
        "|                  Questions                 |     2     |\n",
        "|             Process Description            |     4     |\n",
        "|  **Part 3:   Observations/Interpretation** |   **3**   |\n",
        "|           **Part 4: Reflection**           |   **2**   |\n",
        "|                  **Total**                 |   **30**  |\n",
        "|                                            |           |\n",
        "|                  **Bonus**                 |           |\n",
        "|         **Part 5: Bonus Question**         |   **4**   |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c6de86",
      "metadata": {
        "id": "f7c6de86"
      },
      "source": [
        "# **Part 1: Classification (14.5 marks total)**\n",
        "\n",
        "|                **Question**                | **Point** |\n",
        "|:------------------------------------------:|:---------:|\n",
        "|         **Part 1: Classification**         |           |\n",
        "|          Step 0: Import Libraries          |           |\n",
        "|             Step 1: Data Input             |     1     |\n",
        "|           Step 2: Data Processing          |    1.5    |\n",
        "| Step 3: Implement Machine Learning   Model |           |\n",
        "|           Step 4: Validate Model           |           |\n",
        "|          Step 5: Visualize Results         |     4     |\n",
        "|                  Questions                 |     4     |\n",
        "|             Process Description            |     4     |\n",
        "|                  **Total**                 |  **14.5** |\n",
        "\n",
        "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3c6fc8",
      "metadata": {
        "id": "7e3c6fc8"
      },
      "source": [
        "## **Step 0:** Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "33f86925",
      "metadata": {
        "id": "33f86925"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9d33a8",
      "metadata": {
        "id": "5f9d33a8"
      },
      "source": [
        "## **Step 1:** Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
        "\n",
        "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "33583c67",
      "metadata": {
        "id": "33583c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "262200 4600\n",
            "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Import spam dataset from yellowbrick library\n",
        "# TO DO: Print size and type of X and y\n",
        "\n",
        "# Not working so:\n",
        "from yellowbrick.datasets import load_spam\n",
        "X, y = load_spam(data_home=None, return_dataset=False)\n",
        "\n",
        "print(X.size, y.size)\n",
        "print(type(X), type(y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156db208",
      "metadata": {
        "id": "156db208"
      },
      "source": [
        "## **Step 2:** Data Processing (1.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4e7204f5",
      "metadata": {
        "id": "4e7204f5"
      },
      "outputs": [],
      "source": [
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "if (X.isnull().values.any() == True):\n",
        "    print(\"There are missing values in the dataset. Filling them in with the mean.\")\n",
        "    X = X.fillna(X.mean())\n",
        "    \n",
        "if (y.isnull().values.any() == True):\n",
        "    print(\"There are missing values in the dataset. Filling them in with the mean.\")\n",
        "    y = y.fillna(y.mean())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a489285a",
      "metadata": {
        "id": "a489285a"
      },
      "source": [
        "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f9bc4a23",
      "metadata": {
        "id": "f9bc4a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13110 230\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>...</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_#</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.031</td>\n",
              "      <td>0.376</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.125</td>\n",
              "      <td>3.338</td>\n",
              "      <td>157</td>\n",
              "      <td>611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.05</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.019</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.099</td>\n",
              "      <td>0.079</td>\n",
              "      <td>0.009</td>\n",
              "      <td>4.906</td>\n",
              "      <td>95</td>\n",
              "      <td>1310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3272</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.602</td>\n",
              "      <td>4.700</td>\n",
              "      <td>23</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2213</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.73</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.051</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.103</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.411</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2942</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.31</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.177</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.053</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.744</td>\n",
              "      <td>29</td>\n",
              "      <td>417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2320</th>\n",
              "      <td>0.37</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.107</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.193</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.181</td>\n",
              "      <td>4</td>\n",
              "      <td>104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3173</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3394</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.043</td>\n",
              "      <td>2.983</td>\n",
              "      <td>40</td>\n",
              "      <td>179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3470</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.400</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4191</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2.939</td>\n",
              "      <td>51</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>230 rows × 57 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
              "2015            0.00               0.00           0.29           0.0   \n",
              "1019            0.05               0.05           0.40           0.0   \n",
              "3272            0.00               0.00           0.00           0.0   \n",
              "2213            0.00               0.00           0.28           0.0   \n",
              "2942            0.10               0.10           0.10           0.0   \n",
              "...              ...                ...            ...           ...   \n",
              "2320            0.37               0.00           0.63           0.0   \n",
              "3173            0.00               0.00           0.00           0.0   \n",
              "3394            0.00               0.00           0.00           0.0   \n",
              "3470            0.00               0.00           0.00           0.0   \n",
              "4191            0.00               0.00           0.00           0.0   \n",
              "\n",
              "      word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
              "2015           0.29            0.00               0.0                 0.0   \n",
              "1019           0.34            0.00               0.0                 0.0   \n",
              "3272           0.00            0.00               0.0                 0.0   \n",
              "2213           1.73            0.00               0.0                 0.0   \n",
              "2942           0.21            0.10               0.0                 0.0   \n",
              "...             ...             ...               ...                 ...   \n",
              "2320           0.25            0.12               0.0                 0.0   \n",
              "3173           0.00            0.00               0.0                 0.0   \n",
              "3394           0.00            0.00               0.0                 0.0   \n",
              "3470           0.00            0.00               0.0                 0.0   \n",
              "4191           0.00            0.00               0.0                 0.0   \n",
              "\n",
              "      word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
              "2015             0.29            0.00  ...                   0.0        0.000   \n",
              "1019             0.57            0.05  ...                   0.0        0.019   \n",
              "3272             0.00            0.00  ...                   0.0        0.000   \n",
              "2213             0.00            0.00  ...                   0.0        0.000   \n",
              "2942             0.10            0.31  ...                   0.0        0.035   \n",
              "...               ...             ...  ...                   ...          ...   \n",
              "2320             0.00            0.00  ...                   0.0        0.000   \n",
              "3173             0.00            0.00  ...                   0.0        0.000   \n",
              "3394             0.00            0.00  ...                   0.0        0.000   \n",
              "3470             0.00            0.00  ...                  10.0        0.000   \n",
              "4191             0.00            1.43  ...                   0.0        0.000   \n",
              "\n",
              "      char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
              "2015        0.156        0.031        0.376        0.000        0.125   \n",
              "1019        0.099        0.000        0.099        0.079        0.009   \n",
              "3272        0.000        0.000        0.000        0.000        0.602   \n",
              "2213        0.051        0.000        0.103        0.000        0.000   \n",
              "2942        0.177        0.035        0.070        0.053        0.000   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "2320        0.107        0.000        0.193        0.000        0.000   \n",
              "3173        0.000        0.000        0.000        0.000        0.000   \n",
              "3394        0.130        0.065        0.065        0.000        1.043   \n",
              "3470        0.000        0.000        0.000        0.000        0.000   \n",
              "4191        0.000        0.000        0.000        0.000        0.000   \n",
              "\n",
              "      capital_run_length_average  capital_run_length_longest  \\\n",
              "2015                       3.338                         157   \n",
              "1019                       4.906                          95   \n",
              "3272                       4.700                          23   \n",
              "2213                       1.411                           4   \n",
              "2942                       1.744                          29   \n",
              "...                          ...                         ...   \n",
              "2320                       1.181                           4   \n",
              "3173                       3.000                           5   \n",
              "3394                       2.983                          40   \n",
              "3470                       1.400                           3   \n",
              "4191                       2.939                          51   \n",
              "\n",
              "      capital_run_length_total  \n",
              "2015                       611  \n",
              "1019                      1310  \n",
              "3272                        47  \n",
              "2213                        24  \n",
              "2942                       417  \n",
              "...                        ...  \n",
              "2320                       104  \n",
              "3173                        15  \n",
              "3394                       179  \n",
              "3470                         7  \n",
              "4191                        97  \n",
              "\n",
              "[230 rows x 57 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TO DO: Create X_small and y_small\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_small, _, y_small, _ = train_test_split(X, y, test_size=0.95)\n",
        "print(X_small.size, y_small.size)\n",
        "X_small"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e6c46f",
      "metadata": {
        "id": "70e6c46f"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model\n",
        "\n",
        "1. Import `LogisticRegression` from sklearn\n",
        "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
        "3. Implement the machine learning model with three different datasets:\n",
        "    - `X` and `y`\n",
        "    - Only first two columns of `X` and `y`\n",
        "    - `X_small` and `y_small`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b89f3d84",
      "metadata": {
        "id": "b89f3d84"
      },
      "source": [
        "## **Step 4:** Validate Model\n",
        "\n",
        "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "352106a3",
      "metadata": {
        "id": "352106a3"
      },
      "source": [
        "## **Step 5:** Visualize Results (4 marks for steps 3-5)\n",
        "\n",
        "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
        "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "be4b5c0a",
      "metadata": {
        "id": "be4b5c0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\aarsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data size</th>\n",
              "      <th>Training accuracy</th>\n",
              "      <th>Validation accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>262200.0</td>\n",
              "      <td>0.928696</td>\n",
              "      <td>0.938261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9200.0</td>\n",
              "      <td>0.608406</td>\n",
              "      <td>0.613043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13110.0</td>\n",
              "      <td>0.953488</td>\n",
              "      <td>0.965517</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Data size  Training accuracy  Validation accuracy\n",
              "0   262200.0           0.928696             0.938261\n",
              "1     9200.0           0.608406             0.613043\n",
              "2    13110.0           0.953488             0.965517"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "results = pd.DataFrame(columns=['Data size', 'Training accuracy', 'Validation accuracy'])\n",
        "\n",
        "# Define the datasets\n",
        "datasets = [\n",
        "    (X, y),\n",
        "    (X.iloc[:, :2], y),\n",
        "    (X_small, y_small)\n",
        "]\n",
        "\n",
        "# Process each dataset\n",
        "for i, (X_data, y_data) in enumerate(datasets):\n",
        "    # Step 3:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, random_state=0)\n",
        "    logistic_model = LogisticRegression(max_iter=2000)\n",
        "    logistic_model.fit(X_train, y_train)\n",
        "\n",
        "    # Step 4:\n",
        "    training_accuracy = logistic_model.score(X_train, y_train)\n",
        "    test_accuracy = logistic_model.score(X_test, y_test)\n",
        "\n",
        "    # Step 5:\n",
        "    results.loc[i] = [X_data.size, training_accuracy, test_accuracy]\n",
        "\n",
        "results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4427d4f",
      "metadata": {
        "id": "d4427d4f"
      },
      "source": [
        "## **Questions (4 marks)**\n",
        "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
        "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
        "\n",
        "<font color='Green'><b>YOUR ANSWERS HERE</b></font>\n",
        "\n",
        "**Note values may differ when running the code**\n",
        "\n",
        "1. The training and validation accuracy generally increases when the amount of data increases. Here the model with a size of 9200 had an accuracy of 62.1% and 60.2% respectively, compared to a data size of 13110, there was an accuracy of 93.6% and 93.1% respectively. When comparing the data with a size of 262200, the accuracy is quite similar to the dataset with 13110 for the training accuracy (92.9% to 94.7%). This is because the data for the smaller set is seemingly overfit, thus it will work well because it's super specific to that same data. However, the validation accuracy goes down from 93.8% to 86.2% from the large data set to the smaller one. This intuitively makes sense because there is less data, thus the model has seen less, and it overfits the smaller dataset. So in general to increase accuracy you need more **clean/high quality** data. Reasonably speaking you would want a large variety of data paired with a large magnitude so that you don't overfit. Similarly, if you had a smaller dataset you wouldn't want too much variety (and thus you should clip outliers) because that would also result in overfitting. \n",
        "\n",
        "2. This dataset describes whether an email is spam, thus a false positive would be a legitimate email incorrectly classified as spam. On the other hand false negative, would be a spam email that is incorrectly classified as legitimate. It generally depends, but in this case, it seems as if the convenience or user experience is a priority (e.g., spam detection), a false positive (blocking a legitimate email) might be considered worse because it could obstruct important communications. But if a false negative occurs, it wouldn't be as much of a big deal because as humans we will see it and just move it to our spam folder manually."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7559517a",
      "metadata": {
        "id": "7559517a"
      },
      "source": [
        "## **Process Description (4 marks)**\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fe687f",
      "metadata": {
        "id": "59fe687f"
      },
      "source": [
        "<font color='Green'><b>DESCRIBE YOUR PROCESS HERE</b></font>\n",
        "\n",
        "My code was sourced with a combination of methods, including sklearn documentation (https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html), the linear regression lab 2 and Github CoPilot. I used the lab to understand the flow of code and then cross-referenced with the docs to make use of the logistic regression instead. For my use of Github Co-Pliot, I prompted the code completions with comments that were written based on my research and the steps outlined in the lab document. I had to slightly modify code for small parameters, but the code completions were generally quite effective. \n",
        "\n",
        "I completed the steps in order until step 3, then I did 3-5 for one dataset to understand it fully. Then I abstracted it into a loop so that I could iterate and perform steps 3-5 for each specified data set. \n",
        "\n",
        "The majority of the challenges with this section of the lab surround the concepts of logistic regression and understanding the relevance of different indicators in the data. However, I was able to be successful by referencing the slides from class and the previous labs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb4c78a8",
      "metadata": {
        "id": "fb4c78a8"
      },
      "source": [
        "# **Part 2: Regression (10.5 marks total)**\n",
        "\n",
        "| **Question**                               | **Point** |\n",
        "|--------------------------------------------|-----------|\n",
        "| **Part 2: Regression**                     |           |\n",
        "| Step 1: Data Input                         | 1         |\n",
        "| Step 2: Data Processing                    | 0.5       |\n",
        "| Step 3: Implement Machine Learning   Model | 1         |\n",
        "| Step 4: Validate Mode                      | 1         |\n",
        "| Step 5: Visualize Results                  | 1         |\n",
        "| Questions                                  | 2         |\n",
        "| Process Description                        | 4         |\n",
        "| **Total**                                  | **10.5**  |\n",
        "\n",
        "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ba83c5",
      "metadata": {
        "id": "b2ba83c5"
      },
      "source": [
        "## **Step 1:** Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
        "\n",
        "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6ff2e34f",
      "metadata": {
        "id": "6ff2e34f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8240 1030\n",
            "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Import spam dataset from yellowbrick library\n",
        "# TO DO: Print size and type of X and y\n",
        "from yellowbrick.datasets import load_concrete\n",
        "\n",
        "X, y = load_concrete(data_home=None, return_dataset=False)\n",
        "print(X.size, y.size)\n",
        "print(type(X), type(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5294cfa",
      "metadata": {
        "id": "c5294cfa"
      },
      "source": [
        "## **Step 2:** Data Processing (0.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "693c5fa3",
      "metadata": {
        "id": "693c5fa3"
      },
      "outputs": [],
      "source": [
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "if (X.isnull().values.any() == True):\n",
        "    print(\"There are missing values in the dataset. Filling them in with the mean.\")\n",
        "    X = X.fillna(X.mean())\n",
        "    \n",
        "if (y.isnull().values.any() == True):\n",
        "    print(\"There are missing values in the dataset. Filling them in with the mean.\")\n",
        "    y = y.fillna(y.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc60489",
      "metadata": {
        "id": "1bc60489"
      },
      "source": [
        "## **Step 3:** Implement Machine Learning Model (1 mark)\n",
        "\n",
        "1. Import `LinearRegression` from sklearn\n",
        "2. Instantiate model `LinearRegression()`.\n",
        "3. Implement the machine learning model with `X` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b5041945",
      "metadata": {
        "id": "b5041945"
      },
      "outputs": [],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "lr = LinearRegression()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "lm = lr.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de28482",
      "metadata": {
        "id": "1de28482"
      },
      "source": [
        "## **Step 4:** Validate Model (1 mark)\n",
        "\n",
        "Calculate the training and validation accuracy using mean squared error and R2 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "970c038b",
      "metadata": {
        "id": "970c038b"
      },
      "outputs": [],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "y_train_pred = lm.predict(X_train)\n",
        "y_test_pred = lm.predict(X_test)\n",
        "\n",
        "# Calculate MSE and R2 for training data\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "r2_train = r2_score(y_train, y_train_pred)\n",
        "\n",
        "# Calculate MSE and R2 for testing data\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "r2_test = r2_score(y_test, y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54aa7795",
      "metadata": {
        "id": "54aa7795"
      },
      "source": [
        "## **Step 5:** Visualize Results (1 mark)\n",
        "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
        "2. Add the accuracy results to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "88d223f3",
      "metadata": {
        "id": "88d223f3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training accuracy</th>\n",
              "      <th>Validation accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>MSE</th>\n",
              "      <td>111.358439</td>\n",
              "      <td>95.904136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>R2 score</th>\n",
              "      <td>0.610823</td>\n",
              "      <td>0.623414</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Training accuracy  Validation accuracy\n",
              "MSE              111.358439            95.904136\n",
              "R2 score           0.610823             0.623414"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "results = pd.DataFrame(index=['MSE', 'R2 score'],columns=['Training accuracy', 'Validation accuracy'])\n",
        "results['Training accuracy'] = [mse_train, r2_train]\n",
        "results['Validation accuracy'] = [mse_test, r2_test]\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a42bda",
      "metadata": {
        "id": "70a42bda"
      },
      "source": [
        "## **Questions (2 marks)**\n",
        "\n",
        "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
        "\n",
        "No, using a linear model didn't produce good results, as the R2 score training and validation accuracy values were 61.1 and 62.3% respectively. This indicates that our regression only explains 62.3% of the variation in our y variable. In addition the MSE values for the training and validation accuracy are 111.4 and 95.9 respectively, this indicates a high dispersion around the mean and thus around our function. This is because the data points in y range between 30 and 80, and since both MSE values are greater than the values themselves it causes a significant discrepancy between the predicted and actual values. Particularly the combination of high Mean Squared Error (MSE) values and modest R^2 scores for both training and validation—can be indicative of underfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca0ff2f",
      "metadata": {
        "id": "2ca0ff2f"
      },
      "source": [
        "## **Process Description (4 marks)**\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfdb0880",
      "metadata": {
        "id": "dfdb0880"
      },
      "source": [
        "<font color='Green'><b>Explain YOUR PROCESS here:</b></font>\n",
        "\n",
        "Similar to the first part I used a combination of resources. For this one specifically, I primarily used the sklearn LinearRegression class documentation, the code straight from lab 2 since it was also linear regression, and ChatGPT to fill in the gaps. For the generative AI portion of my code, my prompts were quite basic, they followed the instructions pretty closely, but I had to describe to it the purpose of certain things for the AI to understand. I completed the steps pretty much in order but I found it helpful to go to the first section actually and compare and contrast what was being done there and then apply it to a linear problem. \n",
        "\n",
        "Most of the challenges surrounding the second part came from interpreting what it meant to \"calculate the training and validation accuracy using mean squared error and R2 score.\" I initially understood this statement in a different way, and I passed the function 'mean_squared_error()' and 'r2_score()' the raw x_train and y_train (as well as test data respectively), but then I asked ChatGPT what it meant and it explained to me that I actually had to use the model to come up with predictions and cross reference that with the actual data. I then changed the code to reflect that using .predict."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72ac3eb",
      "metadata": {
        "id": "e72ac3eb"
      },
      "source": [
        "# **Part 3: Observations/Interpretation (3 marks)**\n",
        "\n",
        "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
        "\n",
        "\n",
        "<font color='Green'><b>ADD YOUR FINDINGS HERE</b></font>\n",
        "\n",
        "Comparing the implementations of Logistic and Linear Regression models, it appears there are few similarities in their outcomes. In logistic regression, the focus is on creating a curve that models a number of features, utilizing confusion matrices and metrics like precision to assess the model's effectiveness. We also explored the importance of slightly adjusting a training model in classification problems (notably in binary classifications) to ensure errors are more likely to occur on a more forgiving side. This concept was demonstrated in Part 1, highlighting the significance of differentiating between False Positives and False Negatives and understanding the context of the data's application. For instance, an accuracy of 93.8% is impressive; however, if the 6.2% failure rate consistently results in false positives, that would be problematic. \n",
        "\n",
        "Conversely, in our discussions on linear regression, we examined how loss functions can serve as an estimate to gauge how well a machine learning model fits a specific dataset. The main functions discussed were mean squared error (MSE) and R-squared score, which are interpreted differently. MSE quantifies the fit of a regression model by measuring the average squared difference between the predicted and actual values. Therefore, an MSE value of 95.904136 does not convey much about our dataset without context. Classroom discussions emphasized that the magnitudes of the 'y' parameter values in the dataset are crucial. For instance, if the 'y' values were on the order of 10^4, then an MSE of 95 would imply high accuracy due to the smaller magnitude in comparison. This, however, contrasts with the actual smaller magnitude of the 'y' values in our dataset.\n",
        "\n",
        "Nonetheless, both methodologies provide insights into how well a model fits a dataset and addresses a machine learning problem. Through various metrics (e.g., accuracies for different datasets like 262,200 data points with an accuracy of 0.938261, or an R-squared score of 0.623414), we are able to gauge our ability to predict the 'y' parameter for varying data with changes in 'x'. Fundamentally, this reflects our learning and practice from the lectures: predicting 'y' using 'x' parameters with lines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b84eed",
      "metadata": {
        "id": "40b84eed"
      },
      "source": [
        "# **Part 4: Reflection (2 marks)**\n",
        "Include a sentence or two about:\n",
        "- what you liked or disliked,\n",
        "- found interesting, confusing, challangeing, motivating\n",
        "while working on this assignment.\n",
        "\n",
        "\n",
        "<font color='Green'><b>ADD YOUR THOUGHTS HERE</b></font>\n",
        "\n",
        "I like the tangibility of actually using a simple dataset and ML concepts in modeling, predicting, and analyzing the accuracy of something. In class, I find the concepts to be very abstract, and thus it is hard to see the vision for how it would be applied in a real-life situation. This assignment makes it those concepts more concrete and helps me tie what I have learned in class to something practical. What I disliked was that in certain instructions it was a bit ambiguous as to what we were meant to do, and what the exact purpose of it was. For example, in the first part where it wanted us to split the set into 5% of the original set, it was ambiguous as to if we were supposed to split it again when were creating and validating the model. This was challenging as it forced me to make an assumption based on my interpretation of what the instructions say and on what I have learned in class."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db951b3a",
      "metadata": {
        "id": "db951b3a"
      },
      "source": [
        "# **Part 5: Bonus Question (4 marks)**\n",
        "\n",
        "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
        "\n",
        "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "47623d44",
      "metadata": {
        "id": "47623d44"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Alpha        100.000000\n",
              " MSE_Train    111.358548\n",
              " R2_Train       0.610823\n",
              " MSE_Test      95.894268\n",
              " R2_Test        0.623453\n",
              " Name: 5, dtype: float64,\n",
              " Alpha         10.000000\n",
              " MSE_Train    113.220780\n",
              " R2_Train       0.604314\n",
              " MSE_Test      95.048607\n",
              " R2_Test        0.626774\n",
              " Name: 4, dtype: float64)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "import numpy as np\n",
        "\n",
        "# Define a range of alpha values over a logarithmic scale\n",
        "alpha_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "# Initialize lists to store results\n",
        "ridge_results = []\n",
        "lasso_results = []\n",
        "\n",
        "# Loop over alpha values for Ridge regression\n",
        "for alpha in alpha_values:\n",
        "    ridge_model = Ridge(alpha=alpha)\n",
        "    ridge_model.fit(X_train, y_train)\n",
        "    y_train_pred = ridge_model.predict(X_train)\n",
        "    y_test_pred = ridge_model.predict(X_test)\n",
        "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "    r2_train = r2_score(y_train, y_train_pred)\n",
        "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "    r2_test = r2_score(y_test, y_test_pred)\n",
        "    ridge_results.append([alpha, mse_train, r2_train, mse_test, r2_test])\n",
        "\n",
        "# Loop over alpha values for Lasso regression\n",
        "for alpha in alpha_values:\n",
        "    lasso_model = Lasso(alpha=alpha)\n",
        "    lasso_model.fit(X_train, y_train)\n",
        "    y_train_pred = lasso_model.predict(X_train)\n",
        "    y_test_pred = lasso_model.predict(X_test)\n",
        "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "    r2_train = r2_score(y_train, y_train_pred)\n",
        "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "    r2_test = r2_score(y_test, y_test_pred)\n",
        "    lasso_results.append([alpha, mse_train, r2_train, mse_test, r2_test])\n",
        "\n",
        "# Convert results to DataFrame for easier analysis\n",
        "ridge_df = pd.DataFrame(ridge_results, columns=['Alpha', 'MSE_Train', 'R2_Train', 'MSE_Test', 'R2_Test'])\n",
        "lasso_df = pd.DataFrame(lasso_results, columns=['Alpha', 'MSE_Train', 'R2_Train', 'MSE_Test', 'R2_Test'])\n",
        "\n",
        "# Find the best R2 score and corresponding alpha for Ridge and Lasso\n",
        "best_ridge = ridge_df.loc[ridge_df['R2_Test'].idxmax()]\n",
        "best_lasso = lasso_df.loc[lasso_df['R2_Test'].idxmax()]\n",
        "\n",
        "best_ridge, best_lasso"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b606236",
      "metadata": {
        "id": "1b606236"
      },
      "source": [
        "<font color='Green'><b>ADD YOUR ANSWER HERE</b></font>\n",
        "\n",
        "It seems as if the Ridge and Lasso models were almost identical for their best performance, in terms of R2_Test, but in general all metrics. Lasso had a slight edge with a R2_Test value of 62.7% compare to Ridge with 62.3%. Ridges best values was a result of alpha=100, whereas for Lasso alpha=10. In general this is not 'good enough' the values are very similar to above using a normal Linear Regression model, and as described the dispersion and underfitting is too severe to be hopeful in accurate results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
